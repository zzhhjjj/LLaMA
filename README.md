# LLaMA

Welcome to **LLaMA**, my custom library for training and fine-tuning the LLaMA model.

## Features

Currently, this library supports:

1. Flash Attention, Triton RMSNorm, Flash RoPE (Triton/CUDA acceleration)
2. KV Cache
3. Tensor Parallelism

## Experience
1. Speedup/Loss benchmark results under LLaMA/tools/benchmark

### Coming Soon

I'm actively working on integrating the following features:

1. **Tensor Parallelism**: 
2. **Pipeline Parallelism**
3. **Data Parallelism**
4. **Training on real data**
5. **More benchmarks**
