{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "device ='cuda'\n",
    "\n",
    "def get_cos_sin(seq_length, head_dim, theta=3.14):\n",
    "    # take sequence length and head dimension as input.\n",
    "    # return (seq_length, head_dim)\n",
    "    assert head_dim%2==0\n",
    "    i = torch.arange(head_dim//2).unsqueeze(0).to(device)\n",
    "    position = torch.arange(seq_length).unsqueeze(1).to(device)\n",
    "    angle = 1.0/(theta**(-2*i/head_dim))\n",
    "    return torch.cos(position*angle).repeat(1,2), torch.sin(position*angle).repeat(1,2)\n",
    "\n",
    "def attention(q,k,v,is_causal=True,mask=None):\n",
    "    # take q,k,v of shape (batch_size, num_head, seq_length, head_dim) \n",
    "    assert len(q.shape)==4 \n",
    "    assert is_causal==True or mask!=None\n",
    "    batch_size, num_head, seq_length, head_dim = q.shape\n",
    "    if is_causal:\n",
    "        mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool().to(device)\n",
    "    scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(q.size(-1)))\n",
    "    scaled_dot_product.masked_fill_(mask, float('-inf'))\n",
    "    attention_weights = torch.nn.functional.softmax(scaled_dot_product, dim=-1)\n",
    "    return torch.matmul(attention_weights, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal attention\n",
    "torch.manual_seed(42)\n",
    "batch_size=1\n",
    "seq_length=12\n",
    "hidden_dim=128\n",
    "num_head=4\n",
    "# num_kv_head=4\n",
    "assert hidden_dim%num_head==0\n",
    "head_dim=hidden_dim//num_head\n",
    "\n",
    "cos, sin = get_cos_sin(seq_length, head_dim)\n",
    "x = torch.randn(batch_size,seq_length, hidden_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    # [x1,x2,x3,x4] -> [x3,x4,x1,x2]\n",
    "    head_size = x.shape[-1]\n",
    "    assert head_size%2==0\n",
    "    x1 = x[..., : head_size // 2]  \n",
    "    x2 = x[..., head_size // 2 :]  \n",
    "    return torch.cat([-x2, x1], dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(x, cos, sin):\n",
    "    # take x of shape (batch_size, num_heads, seq_length, head_dim)\n",
    "    # cos, sin of shape (seq_length, head_dim)\n",
    "    batch_size, num_head, seq_length, head_dim = x.size()\n",
    "    assert cos.size(0)==seq_length\n",
    "    assert cos.size(1)==head_dim\n",
    "    x = x * cos + rotate_half(x) * sin\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3027,  0.2380,  0.6034,  ...,  0.5112,  0.7894, -0.6095],\n",
       "         [-0.0117, -0.0066,  0.8311,  ...,  0.5305,  0.6811, -0.3407],\n",
       "         [-0.2062,  0.0031,  0.3447,  ...,  0.2466,  0.3350, -0.1707],\n",
       "         ...,\n",
       "         [ 0.2361, -0.0146, -0.0264,  ..., -0.0757,  0.0509, -0.3383],\n",
       "         [ 0.1223,  0.0140,  0.0098,  ..., -0.1104,  0.0818, -0.3325],\n",
       "         [ 0.2322, -0.0486, -0.0757,  ..., -0.1011, -0.0754, -0.3290]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def CausalAttention(x,cos,sin):\n",
    "    # take x of shape (batch_size, seq_length, hidden_dim)\n",
    "    # return (batch_size, seq_length, hidden_dim)\n",
    "    batch_size, seq_length, hidden_dim = x.shape\n",
    "    query_prj = torch.nn.Linear(hidden_dim, hidden_dim).to(device)\n",
    "    key_prj = torch.nn.Linear(hidden_dim, hidden_dim).to(device)\n",
    "    value_prj = torch.nn.Linear(hidden_dim, hidden_dim).to(device)\n",
    "    \n",
    "    head_dim = hidden_dim//num_head\n",
    "    query = query_prj(x).view(batch_size,seq_length, num_head, head_dim)\n",
    "    value = value_prj(x).view(batch_size,seq_length, num_head, head_dim)\n",
    "    key = key_prj(x).view(batch_size,seq_length, num_head, head_dim)\n",
    "    query = query.transpose(1, 2) # batch_size, num_head, seq_length, head_dim\n",
    "    key = key.transpose(1, 2)\n",
    "    value = value.transpose(1, 2)\n",
    "    query = apply_rotary_pos_emb(query, cos, sin)\n",
    "    key = apply_rotary_pos_emb(key, cos, sin)\n",
    "    out = attention(query, key, value).transpose(1, 2).contiguous().view(batch_size,seq_length, hidden_dim)\n",
    "    ref_out = torch.nn.functional.scaled_dot_product_attention(query, key, value,is_causal=True).transpose(1, 2).contiguous().view(batch_size,seq_length, hidden_dim)\n",
    "    torch.testing.assert_close(ref_out, out, rtol=1e-6, atol=1e-6)\n",
    "    # print(query.size(), key.size(), value.size())\n",
    "    # print(out.size())\n",
    "    return out\n",
    "\n",
    "CausalAttention(x,cos,sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch.nn as nn\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    batch_size: int = 1\n",
    "    seq_length: int = 12\n",
    "    hidden_dim: int = 128\n",
    "    num_heads: int = 4\n",
    "    num_layers: int = 16\n",
    "\n",
    "config = Config()\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim=128):\n",
    "        self.layer_norm1 = torch.nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm2 = torch.nn.LayerNorm(hidden_dim)\n",
    "        self.attention = CausalAttention\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 4*hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*hidden_dim, hidden_dim)\n",
    "        )\n",
    "    def forward(self, x, cos, sin):\n",
    "        out = self.attention(self.layer_norm1(x),cos,sin)\n",
    "        out = self.mlp(self.layer_norm2(out))\n",
    "        return out\n",
    "    \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        self.num_heads = config.num_heads\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.seq_length = config.seq_length\n",
    "        self.num_layers = config.num_layers\n",
    "        self.layers = nn.ModuleList([DecoderLayer(self.hidden_dim) for _ in range(self.num_layers)])\n",
    "        self.cos, self.sin = get_cos_sin(self.seq_length, self.hidden_dim//self.num_heads)\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, self.cos, self.sin)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nt_mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
